{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, GRU, Multiply, Reshape\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "# from tcn import TCN\n",
    "%matplotlib inline\n",
    "from keras.layers import merge, Input, Dense, TimeDistributed, Lambda                                   \n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, GlobalMaxPooling2D,MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Input, GlobalAveragePooling2D,AveragePooling2D, Add\n",
    "from keras.optimizers import Adam, rmsprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Activation, Lambda\n",
    "from keras.layers import Conv1D, SpatialDropout1D\n",
    "from keras.layers import Convolution1D, Dense\n",
    "MAX_SENT_LENGTH = 55\n",
    "MAX_SENTS = 1\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "futrue_time=1\n",
    "\n",
    "time_step=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_normalization(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\" Normalize a layer to the maximum activation\n",
    "\n",
    "    This keeps a layers values between zero and one.\n",
    "    It helps with relu's unbounded activation\n",
    "\n",
    "    Args:\n",
    "        x: The layer to normalize\n",
    "\n",
    "    Returns:\n",
    "        A maximal normalized layer\n",
    "    \"\"\"\n",
    "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
    "    out = x / max_values\n",
    "    return out\n",
    "\n",
    "\n",
    "def wave_net_activation(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\"This method defines the activation used for WaveNet\n",
    "\n",
    "    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n",
    "\n",
    "    Args:\n",
    "        x: The layer we want to apply the activation to\n",
    "\n",
    "    Returns:\n",
    "        A new layer with the wavenet activation applied\n",
    "    \"\"\"\n",
    "    tanh_out = Activation('tanh')(x)\n",
    "    sigm_out = Activation('sigmoid')(x)\n",
    "    return keras.layers.multiply([tanh_out, sigm_out])\n",
    "\n",
    "\n",
    "def residual_block(x, s, i, c, activation, nb_filters, kernel_size, padding, dropout_rate=0):\n",
    "    # type: (Layer, int, int, int, str, int, int, str, float, str) -> Tuple[Layer, Layer]\n",
    "    \"\"\"Defines the residual block for the WaveNet TCN\n",
    "\n",
    "    Args:\n",
    "        x: The previous layer in the model\n",
    "        s: The stack index i.e. which stack in the overall TCN\n",
    "        i: The dilation power of 2 we are using for this residual block\n",
    "        c: The dilation name to make it unique. In case we have same dilation twice: [1, 1, 2, 4].\n",
    "        activation: The name of the type of activation to use\n",
    "        nb_filters: The number of convolutional filters to use in this block\n",
    "        kernel_size: The size of the convolutional kernel\n",
    "        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "    Returns:\n",
    "        A tuple where the first element is the residual model layer, and the second\n",
    "        is the skip connection.\n",
    "    \"\"\"\n",
    "\n",
    "    original_x = x\n",
    "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding=padding)(x)\n",
    "    if activation == 'norm_relu':\n",
    "        x = Activation('relu')(conv)\n",
    "        x = Lambda(channel_normalization)(x)\n",
    "    elif activation == 'wavenet':\n",
    "        x = wave_net_activation(conv)\n",
    "    else:\n",
    "        x = Activation(activation)(conv)\n",
    "\n",
    "    x = SpatialDropout1D(dropout_rate)(x)\n",
    "\n",
    "    # 1x1 conv.\n",
    "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
    "    res_x = keras.layers.add([original_x, x])\n",
    "    return res_x, x\n",
    "\n",
    "\n",
    "def process_dilations(dilations):\n",
    "    def is_power_of_two(num):\n",
    "        return num != 0 and ((num & (num - 1)) == 0)\n",
    "\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n",
    "        return new_dilations\n",
    "class TCN:\n",
    "    \"\"\"Creates a TCN layer.\n",
    "\n",
    "        Input shape:\n",
    "            A tensor of shape (batch_size, timesteps, input_dim).\n",
    "\n",
    "        Args:\n",
    "            nb_filters: The number of filters to use in the convolutional layers.\n",
    "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "            nb_stacks : The number of stacks of residual blocks to use.\n",
    "            activation: The activations to use (norm_relu, wavenet, relu...).\n",
    "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
    "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
    "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "        Returns:\n",
    "            A TCN layer.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_filters=64,\n",
    "                 kernel_size=2,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=[1, 2, 4, 8, 16, 32],\n",
    "                 activation='norm_relu',\n",
    "                 padding='causal',\n",
    "                 use_skip_connections=True,\n",
    "                 dropout_rate=0.0,\n",
    "                 return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "        self.activation = activation\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "        self.padding = padding\n",
    "\n",
    "        if padding != 'causal' and padding != 'same':\n",
    "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
    "\n",
    "        if not isinstance(nb_filters, int):\n",
    "            # print 'An interface change occurred after the version 2.1.2.'\n",
    "            # print 'Before: tcn.TCN(i, return_sequences=False, ...)'\n",
    "            # print 'Now should be: tcn.TCN(return_sequences=False, ...)(i)'\n",
    "            # print 'Second solution is to pip install keras-tcn==2.1.2 to downgrade.'\n",
    "            raise Exception()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = Convolution1D(self.nb_filters, 1, padding=self.padding)(x)\n",
    "        skip_connections = []\n",
    "        for s in range(self.nb_stacks):\n",
    "            for i, d in enumerate(self.dilations):\n",
    "                x, skip_out = residual_block(x, s, d, i, self.activation, self.nb_filters,\n",
    "                                             self.kernel_size, self.padding, self.dropout_rate)\n",
    "                skip_connections.append(skip_out)\n",
    "        if self.use_skip_connections:\n",
    "            x = keras.layers.add(skip_connections)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            output_slice_index = -1\n",
    "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rf_data=pd.read_csv('',encoding='gb2312')\n",
    "# rf_data=rf_data.drop(rf_data.columns[7],axis=1)\n",
    "# rf_data=rf_data.drop(labels=,'columns')\n",
    "#处理id和时间特征\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "Enc=OrdinalEncoder()\n",
    "x=Enc.fit_transform(np.array(rf_data.iloc[:,0]).reshape(-1,1))\n",
    "# id=rf_data[:,0]\n",
    "# interval1=rf_data.iloc[:,1]\n",
    "# interval2=rf_data[:,2]\n",
    "\n",
    "rf_data.insert(3,\"idco2der\",x)\n",
    "rf_data.insert (4,\"interval1\",[time.localtime(ti).tm_hour for ti in rf_data.iloc[:,1].values])\n",
    "rf_data.insert (5,\"interval2\",[time.localtime(ti).tm_hour for ti in rf_data.iloc[:,2].values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_data.describe().to_csv(\"datainfo.csv\",encoding='gb2312')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data_train=rf_data.iloc[:400,3:]\n",
    "rf_data_test=rf_data.iloc[400: ,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu(x):\n",
    "    x=x/100\n",
    "    return x\n",
    "def  add_df(x):\n",
    "    x = 1 + x\n",
    "    return x\n",
    "\n",
    "target_data=rf_data.iloc[:,10:11]\n",
    "for i in range(time_step-1,25):\n",
    "    target_data['整体成单数'+str(i)]=target_data['999'].shift(-i)\n",
    "td=target_data.iloc[:,1:2]\n",
    "td[\"add1\"]=target_data['整体成单数'+str(time_step)]\n",
    "tdt=td.iloc[:400-time_step,1:2]\n",
    "tdte=td.iloc[400:502-time_step,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zb_data=rf_data_train.iloc[:,:18]\n",
    "gg_data=rf_data_train.iloc[:,21:]\n",
    "intera_data=rf_data_train.iloc[:,18:21]\n",
    "# rf_data_train=std.fit_transform(rf_data_train)\n",
    "# # sd=std.fit_transform(sd)\n",
    "# zb_data=rf_data_train.iloc[:,:]\n",
    "# gg_data=rf_data_train.iloc[:,:]\n",
    "# intera_data=rf_data_train.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step=1\n",
    "gg_m=[]\n",
    "for i in range(1,time_step+1):\n",
    "    locals()['gg_data'+str(i)]=gg_data[i-1:400-time_step-1+i].values\n",
    "    \n",
    "    gg_m.append(locals()['gg_data'+str(i)])\n",
    "    \n",
    "zb_m=[]\n",
    "for i in range(1,time_step+1):\n",
    "    locals()['zb_data'+str(i)]=zb_data[i-1:400-time_step-1+i].values\n",
    "    \n",
    "    zb_m.append(locals()['zb_data'+str(i)])\n",
    "intera_m=[]\n",
    "for i in range(1,time_step+1):\n",
    "    locals()['intera_data'+str(i)]=intera_data[i-1:400-time_step-1+i].values\n",
    "    \n",
    "    intera_m.append(locals()['intera_data'+str(i)])\n",
    "\n",
    "gg_m=(np.array(gg_m).swapaxes(0,1))\n",
    "zb_m=(np.array(zb_m).swapaxes(0,1))\n",
    "intera_m=(np.array(intera_m).swapaxes(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_data_test=std.fit_transform(rf_data_test)\n",
    "# sd=std.fit_transform(sd)\n",
    "zb_data_t=rf_data_test.iloc[:,:18]\n",
    "gg_data_t=rf_data_test.iloc[:,21:]\n",
    "intera_data_t=rf_data_test.iloc[:,18:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# time_step=24\n",
    "gg_mt=[]\n",
    "for i in range(1,time_step+1):\n",
    "    locals()['gg_data_t'+str(i)]=gg_data_t[i-1:102-time_step-1+i].values\n",
    "    \n",
    "    gg_mt.append(locals()['gg_data_t'+str(i)])\n",
    "    \n",
    "zb_mt=[]\n",
    "for i in range(1,time_step+1):\n",
    "    locals()['zb_data_t'+str(i)]=zb_data_t[i-1:102-time_step-1+i].values\n",
    "    \n",
    "    zb_mt.append(locals()['zb_data_t'+str(i)])\n",
    "intera_mt=[]\n",
    "for i in range(1,time_step+1):\n",
    "    locals()['intera_data_t'+str(i)]=intera_data_t[i-1:102-time_step-1+i].values\n",
    "    \n",
    "    intera_mt.append(locals()['intera_data_t'+str(i)])\n",
    "\n",
    "gg_mt=(np.array(gg_mt).swapaxes(0,1))\n",
    "zb_mt=(np.array(zb_mt).swapaxes(0,1))\n",
    "intera_mt=(np.array(intera_mt).swapaxes(0,1))\n",
    "zzz_m=rf_data_train.iloc[time_step:,:].drop(rf_data_train.columns[7],axis=1)\n",
    "zzz_mt=rf_data_test.iloc[time_step:,:].drop(rf_data_test.columns[7],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data_train.columns[:18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_len1=18\n",
    "review_len2=3\n",
    "review_len3=14\n",
    "from attention_keras import Attention\n",
    "from keras.layers import LeakyReLU\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(** kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight', \n",
    "                                 shape=(input_shape[1], input_shape[1]),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "        # x.shape = (batch_size, seq_len, time_steps)\n",
    "        # general\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W)))\n",
    "        a = K.permute_dimensions(a, (0, 2, 1))\n",
    "        outputs = a * inputs\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]\n",
    "\n",
    "def create_model(learn_rate=0.01 ,neurons=time_step ,traffic_n=5):\n",
    "\n",
    "    \n",
    "    inp_we=Input(shape=(review_len2,))\n",
    "\n",
    "    weam=Model(inputs=inp_we, outputs=inp_we)\n",
    "    \n",
    "    \n",
    "#     交通指数数据模块\n",
    "    inp=Input(shape=(review_len3,))\n",
    "\n",
    "#     out=Flatten()(out)\n",
    "    out=Dense(traffic_n)(inp)\n",
    "    tra=Model(inputs=inp, outputs=out)\n",
    "\n",
    "    sentence_input = Input(shape=(review_len1+traffic_n,))\n",
    "    # s_input=keras.layers.concatenate([sentence_input])\n",
    "    embed = Reshape((review_len1+traffic_n,1))(sentence_input)\n",
    "    l_lstm=Attention(8,16)([embed,embed,embed])\n",
    "#     l_lstm = Bidirectional(GRU(32, return_sequences=True))(embed)\n",
    "#     embed = Reshape((54+traffic_n,1))(l_lstm)\n",
    "    l_dense = TimeDistributed(Dense(36))(l_lstm)\n",
    "#     l_dense = Bidirectional(GRU(32, return_sequences=True))(l_lstm)\n",
    "\n",
    "    l_att = AttentionLayer()(l_dense)\n",
    "\n",
    "    sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "    \n",
    "    review_input = Input(shape=(time_step,review_len1))\n",
    "    r_input = Input(shape=(time_step,review_len3))\n",
    "    r_encoder = TimeDistributed(tra)(r_input)\n",
    "    s_input=keras.layers.concatenate([r_encoder,review_input])\n",
    "    \n",
    "    review_encoder = TimeDistributed(sentEncoder)(s_input)\n",
    "    \n",
    "\n",
    "\n",
    "    wea_input=Input(shape=(time_step,review_len2))\n",
    "    wea_encoder = TimeDistributed(weam)(wea_input)\n",
    "    # we_time_encoder=keras.layers.concatenate([wea_input,review_encoder])\n",
    "\n",
    "    l_lstm_sent=TCN(padding='same',return_sequences=True)(review_encoder)\n",
    "    we_time_encoder=keras.layers.concatenate([wea_input,l_lstm_sent])\n",
    "\n",
    "    l_dense_sent = TimeDistributed(Dense(64))(we_time_encoder)\n",
    "#     l_dense_sent = TCN(padding='same',return_sequences=True)(we_time_encoder)\n",
    "#     l_dense_sent = Bidirectional(GRU(32, return_sequences=True))(we_time_encoder)\n",
    "#     l_dense_sent = TimeDistributed(Dense(64))(we_time_encoder)\n",
    "    l_att_sent = AttentionLayer()(l_dense_sent)\n",
    " \n",
    "    l_att_sent=keras.layers.concatenate([l_att_sent,zzz])\n",
    "\n",
    "    l_att_sent= Dense(256)(l_att_sent)\n",
    "    l_att_sent=LeakyReLU()(l_att_sent)\n",
    "    \n",
    "    l_att_sent= Dense(128)(l_att_sent)\n",
    "    l_att_sent=LeakyReLU()(l_att_sent)\n",
    "    l_att_sent= Dense(64)(l_att_sent)\n",
    "    l_att_sent=LeakyReLU()(l_att_sent)\n",
    "    l_att_sent= Dense(32)(l_att_sent)\n",
    "    l_att_sent=LeakyReLU()(l_att_sent)\n",
    "    l_att_sent= Dense(5)(l_att_sent)\n",
    "    l_att_sent=LeakyReLU()(l_att_sent)\n",
    "    l_att_sent= Dense(3)(l_att_sent)\n",
    "    l_att_sent=LeakyReLU()(l_att_sent)\n",
    "    preds = Dense(1)(l_att_sent)\n",
    "#     preds=LeakyReLU()(preds)\n",
    "    model_1 = Model([review_input,wea_input,r_input,zzz], preds)\n",
    "    model_1.compile(optimizer=Adam(lr=learn_rate, beta_1=0.99, beta_2=0.999, decay=0.006),\n",
    "                        loss='mae')\n",
    "    return model_1\n",
    "\n",
    "model=create_model()\n",
    "history=model.fit([zb_m,intera_m,gg_m,zzz_m],tdt, epochs=18000, batch_size=32,verbose=2,validation_split=0.15,shuffle=True)\n",
    "# 绘制训练 & 验证的损失值\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_data_train.columns[18:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data_train.columns[21:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import innvestigate.utils as iutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import innvestigate\n",
    "analyzer = innvestigate.create_analyzer(\"lrp.z\",model,disable_model_checks=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = analyzer.analyze([zb_m,intera_m,gg_m,zzz_m]) #inputs就是模型输入\n",
    "# importance_scores = iutils.analyze_influence(analysis)\n",
    "# importance_scores = iutils.analyze(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(analysis[2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_analysis = np.mean(analysis[2], axis=(0, 1))\n",
    "\n",
    "# Reshape the mean analysis to a 1D vector\n",
    "importance_scores = mean_analysis.reshape(-1)\n",
    "# importance_scores\n",
    "feature_names = [i for i in range(14)]\n",
    "\n",
    "# Create a bar chart of the importance scores\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(feature_names, importance_scores)\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "e = shap.DeepExplainer(model,[zb_m,intera_m,gg_m,zzz_m])\n",
    "# shap_values = explainer([zb_m,intera_m,gg_m,zzz_m])\n",
    "shap_values = e.shap_values([zb_mt[1:5],intera_mt[1:5],gg_mt[1:5],zzz_mt[1:5]])\n",
    "# shap_values=explainer.shap_values\n",
    "# visualize the first prediction's explanation\n",
    "# shap.plots.waterfall(shap_values[0])\n",
    "# shap.force_plot(explainer.expected_value, shap_values, [zb_m,intera_m,gg_m,zzz_m])\n",
    "shap.force_plot(e.expected_value, shap_values[1],[zb_mt[1,:],intera_m[1,:],gg_m[1,:],zzz_m[1,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  openpyxl\n",
    "pt=(tdte)\n",
    "pp=pd.Series(((pre_test)).flatten())\n",
    "pt.to_excel('pt_1_1.5.xlsx',sheet_name='12') \n",
    "pp.to_excel('pp_1_1.5.xlsx',sheet_name='12') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      353.118103\n",
       "1      895.797241\n",
       "2      461.392548\n",
       "3      737.091003\n",
       "4      434.489655\n",
       "          ...    \n",
       "96     732.557861\n",
       "97     421.195801\n",
       "98     680.011353\n",
       "99     388.493164\n",
       "100    306.030975\n",
       "Length: 101, dtype: float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>add1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>869.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>738.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>737.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      add1\n",
       "400  372.0\n",
       "401  869.0\n",
       "402  425.0\n",
       "403  738.0\n",
       "404  437.0\n",
       "..     ...\n",
       "496  737.0\n",
       "497  380.0\n",
       "498  658.0\n",
       "499  337.0\n",
       "500  275.0\n",
       "\n",
       "[101 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def  add_df(x):\n",
    "    x = 1 + x\n",
    "    return x\n",
    "tdte=tdte+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r1_score\n",
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 99\n",
    "\n",
    "mape = mean_absolute_percentage_error(tdte, pre_test)\n",
    "mae = mean_absolute_error(tdte, pre_test)\n",
    "mse = mean_squared_error(tdte, pre_test)\n",
    "rmse = np.sqrt(mse)\n",
    "r1 = r2_score(tdte, pre_test)\n",
    "\n",
    "print(\"MAPE:\", mape)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R1:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_test=model.predict([zb_mt,intera_mt,gg_mt,zzz_mt])\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error \n",
    "print((mean_absolute_percentage_error(tdte,pre_test)),(np.sqrt(mean_squared_error(tdte,pre_test))))\n",
    "# mean_absolute_error(pre_test,td_test[:145]),np.sqrt(mean_squared_error(pre_test,td_test[:145]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 34)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zzz_mt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred,dtype=float).flatten()\n",
    "    # print(type(y_true[0]))\n",
    "    # print(y_pred.shape)\n",
    "    # print(y_true.shape)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('new_LRP_norm03_model_17.762_22.935new_At_TCN_1.5.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=pd.read_excel(r\"/home/zhb/Jdoc/work/pp.xlsx\")\n",
    "pt=pd.read_excel(r\"/home/zhb/Jdoc/work/pt.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'Microsoft YaHei'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "\n",
    "\n",
    "df = pd.concat([gg_data, zb_data])\n",
    "df_coor = df.corr()\n",
    "df_coor.to_csv(\"correlation_matrix.csv\", index=True, encoding='utf-8-sig')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca8b9bb19742d5b6220a7434602d631e4d56cba6afd780858bcdcaed41b73f98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
